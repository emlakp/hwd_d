{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a8bde24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akopyane/rl/lumos\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ac017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = (Path.home() / \"rl\" / \"lumos\").resolve()\n",
    "assert (project_root / \"config\").is_dir(), project_root  # optional sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55781d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets:\n",
      "  vision_dataset:\n",
      "    _target_: lumos.datasets.vision_wm_disk_dataset.VisionWMDiskDataset\n",
      "    key: vis\n",
      "    save_format: npz\n",
      "    use_cached_data: false\n",
      "    reset_prob: ${datamodule.reset_prob}\n",
      "    batch_size: ${datamodule.batch_size}\n",
      "    min_window_size: ${datamodule.seq_len}\n",
      "    max_window_size: ${datamodule.seq_len}\n",
      "    proprio_state: ${datamodule.proprioception_dims}\n",
      "    obs_space: ${datamodule.observation_space}\n",
      "    pad: false\n",
      "    for_wm: true\n",
      "    lang_folder: ''\n",
      "    num_workers: 0\n",
      "transforms:\n",
      "  train:\n",
      "    rgb_static:\n",
      "    - _target_: lumos.utils.transforms.ScaleImageTensor\n",
      "    - _target_: torchvision.transforms.Normalize\n",
      "      mean:\n",
      "      - 0.5\n",
      "      std:\n",
      "      - 0.5\n",
      "    rgb_gripper:\n",
      "    - _target_: lumos.utils.transforms.ScaleImageTensor\n",
      "    - _target_: torchvision.transforms.Normalize\n",
      "      mean:\n",
      "      - 0.5\n",
      "      std:\n",
      "      - 0.5\n",
      "    out_rgb:\n",
      "    - _target_: lumos.utils.transforms.UnNormalizeImageTensor\n",
      "      mean:\n",
      "      - 0.5\n",
      "      std:\n",
      "      - 0.5\n",
      "    robot_obs:\n",
      "    - _target_: lumos.utils.transforms.NormalizeVector\n",
      "  val:\n",
      "    rgb_static:\n",
      "    - _target_: lumos.utils.transforms.ScaleImageTensor\n",
      "    - _target_: torchvision.transforms.Normalize\n",
      "      mean:\n",
      "      - 0.5\n",
      "      std:\n",
      "      - 0.5\n",
      "    rgb_gripper:\n",
      "    - _target_: lumos.utils.transforms.ScaleImageTensor\n",
      "    - _target_: torchvision.transforms.Normalize\n",
      "      mean:\n",
      "      - 0.5\n",
      "      std:\n",
      "      - 0.5\n",
      "    out_rgb:\n",
      "    - _target_: lumos.utils.transforms.UnNormalizeImageTensor\n",
      "      mean:\n",
      "      - 0.5\n",
      "      std:\n",
      "      - 0.5\n",
      "    robot_obs:\n",
      "    - _target_: lumos.utils.transforms.NormalizeVector\n",
      "proprioception_dims:\n",
      "  n_state_obs: 15\n",
      "  keep_indices:\n",
      "  - - 0\n",
      "    - 15\n",
      "  robot_orientation_idx:\n",
      "  - 3\n",
      "  - 6\n",
      "  normalize: true\n",
      "  normalize_robot_orientation: true\n",
      "observation_space:\n",
      "  rgb_obs:\n",
      "  - rgb_static\n",
      "  - rgb_gripper\n",
      "  depth_obs: []\n",
      "  state_obs:\n",
      "  - robot_obs\n",
      "  actions:\n",
      "  - rel_actions\n",
      "  language: []\n",
      "batch_sampler:\n",
      "  _target_: lumos.utils.samplers.EquiSampler\n",
      "  _recursive_: false\n",
      "  data_size: ???\n",
      "  seq_len: ${datamodule.seq_len}\n",
      "  batch_size: ${datamodule.batch_size}\n",
      "  init_idx: null\n",
      "_target_: lumos.datasets.calvin_data_module.CalvinDataModule\n",
      "_recursive_: false\n",
      "root_data_dir: dataset/task_D_D\n",
      "action_space: 7\n",
      "action_max:\n",
      "- 1.0\n",
      "- 1.0\n",
      "- 1.0\n",
      "- 1.0\n",
      "- 1.0\n",
      "- 1.0\n",
      "- 1.0\n",
      "action_min:\n",
      "- -1.0\n",
      "- -1.0\n",
      "- -1.0\n",
      "- -1.0\n",
      "- -1.0\n",
      "- -1.0\n",
      "- -1\n",
      "shuffle_val: false\n",
      "load_feats: false\n",
      "reset_prob: 0.01\n",
      "batch_size: 1\n",
      "seq_len: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â”€ load the Hydra config just like scripts/train_wm.py\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from hydra import initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "project_root = Path(\"/home/akopyane/rl/lumos\").resolve()\n",
    "sys.path.insert(0, project_root.as_posix())\n",
    "\n",
    "with initialize_config_dir(version_base=\"1.3\", config_dir=str(project_root / \"config\")):\n",
    "    cfg = compose(\n",
    "        config_name=\"train_wm\",\n",
    "        overrides = [\n",
    "            \"datamodule.root_data_dir=dataset/task_D_D\",\n",
    "            \"datamodule.batch_size=1\",\n",
    "            \"datamodule.seq_len=10\",\n",
    "            \"datamodule.datasets.vision_dataset.use_cached_data=false\",\n",
    "            \"datamodule.datasets.vision_dataset.num_workers=0\",\n",
    "            \"trainer.accelerator=gpu\",\n",
    "            \"trainer.devices=1\",\n",
    "            # \"trainer.max_steps=500\",\n",
    "            \"trainer.max_epochs=1000\",\n",
    "            \"trainer.log_every_n_steps=1\",\n",
    "            \"trainer.check_val_every_n_epoch=10\",\n",
    "            \"+trainer.overfit_batches=1.0\",\n",
    "            \"+trainer.fast_dev_run=false\",  # make sure this is here\n",
    "            \"datamodule.datasets.vision_dataset._target_=lumos.datasets.vision_wm_disk_dataset.VisionWMDiskDataset\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg.datamodule))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ed05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 â”€ instantiate the transforms exactly like BaseDataModule.setup\n",
    "import hydra\n",
    "import torchvision\n",
    "from lumos.datasets.utils.episode_utils import load_dataset_statistics\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "train_dir = project_root / cfg.datamodule.root_data_dir / \"training\"\n",
    "val_dir = project_root / cfg.datamodule.root_data_dir / \"validation\"\n",
    "\n",
    "transforms_cfg = load_dataset_statistics(train_dir, val_dir, cfg.datamodule.transforms)\n",
    "\n",
    "def build_tfms(branch):\n",
    "    return {\n",
    "        cam: torchvision.transforms.Compose([hydra.utils.instantiate(t) for t in transforms_cfg[branch][cam]])\n",
    "        for cam in transforms_cfg[branch]\n",
    "    }\n",
    "\n",
    "train_tfms = build_tfms(\"train\")\n",
    "val_tfms = build_tfms(\"val\")\n",
    "\n",
    "\n",
    "resize34 = transforms.Lambda(\n",
    "    lambda x: F.interpolate(x, size=(34, 34), mode=\"bilinear\", align_corners=False)\n",
    ")\n",
    "\n",
    "def inject_resize(pipe):\n",
    "    steps = list(pipe.transforms)\n",
    "    return transforms.Compose([steps[0], resize34, *steps[1:]])\n",
    "\n",
    "train_tfms[\"rgb_static\"] = inject_resize(train_tfms[\"rgb_static\"])\n",
    "train_tfms[\"rgb_gripper\"] = inject_resize(train_tfms[\"rgb_gripper\"])\n",
    "val_tfms[\"rgb_static\"] = inject_resize(val_tfms[\"rgb_static\"])\n",
    "val_tfms[\"rgb_gripper\"] = inject_resize(val_tfms[\"rgb_gripper\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b9c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence keys: ['robot_obs', 'rgb_obs', 'depth_obs', 'actions', 'state_info', 'lang', 'reset', 'frame', 'idx']\n",
      "robot_obs       (10, 15) torch.float32\n",
      "lang            (0,) torch.float32\n",
      "reset           (10,) torch.bool\n",
      "frame           (10,) torch.int32\n",
      "\n",
      "Batch shapes:\n",
      "robot_obs       (10, 1, 15) torch.float32\n",
      "lang            (1, 0) torch.float32\n",
      "reset           (10, 1) torch.bool\n",
      "frame           (10, 1) torch.int32\n",
      "idx             (1,) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â”€ build the dataset/dataloader and inspect shapes\n",
    "from torch.utils.data import DataLoader\n",
    "from lumos.datasets.vision_wm_disk_dataset import VisionWMDiskDataset\n",
    "from lumos.utils.nn_utils import transpose_collate_wm\n",
    "\n",
    "dataset_cfg = cfg.datamodule.datasets.vision_dataset\n",
    "\n",
    "train_ds = VisionWMDiskDataset(\n",
    "    datasets_dir=train_dir,\n",
    "    obs_space=cfg.datamodule.observation_space,\n",
    "    proprio_state=cfg.datamodule.proprioception_dims,\n",
    "    key=dataset_cfg.key,\n",
    "    lang_folder=dataset_cfg.lang_folder,\n",
    "    num_workers=dataset_cfg.num_workers,\n",
    "    transforms=train_tfms,\n",
    "    batch_size=cfg.datamodule.batch_size,\n",
    "    min_window_size=cfg.datamodule.seq_len,\n",
    "    max_window_size=cfg.datamodule.seq_len,\n",
    "    pad=dataset_cfg.pad,\n",
    "    for_wm=dataset_cfg.for_wm,\n",
    "    reset_prob=cfg.datamodule.reset_prob,\n",
    "    save_format=dataset_cfg.save_format,\n",
    "    use_cached_data=False,\n",
    ")\n",
    "\n",
    "sample = train_ds[0]\n",
    "print(\"Sequence keys:\", list(sample.keys()))\n",
    "for k, v in sample.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"{k:15s} {tuple(v.shape)} {v.dtype}\")\n",
    "\n",
    "loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.datamodule.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=transpose_collate_wm,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "for k, v in batch.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"{k:15s} {tuple(v.shape)} {v.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae8928a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"actions\"][\"rel_actions\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed0d318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 15])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"robot_obs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829fc162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 3, 34, 34])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"rgb_obs\"][\"rgb_static\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c547ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['robot_obs', 'rgb_obs', 'depth_obs', 'actions', 'state_info', 'lang', 'reset', 'frame', 'idx'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7de061",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_static\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "sample[\"rgb_obs\"][\"rgb_static\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8e0640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akopyane/rl/lumos/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class DebugDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_loader, val_loader):\n",
    "        super().__init__()\n",
    "        self._train_loader = train_loader\n",
    "        self._val_loader = val_loader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return {\"vis\": self._train_loader}\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return {\"vis\": self._val_loader}\n",
    "\n",
    "debug_dm = DebugDataModule(loader, loader)\n",
    "debug_dm.train_transforms = {\"out_rgb\": train_tfms[\"out_rgb\"]}\n",
    "debug_dm.val_transforms = {\"out_rgb\": val_tfms[\"out_rgb\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c8a438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 â”€ instantiate DreamerV2 + Lightning trainer on CPU\n",
    "import contextlib\n",
    "from copy import deepcopy\n",
    "\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from lumos.utils.info_utils import setup_logger, setup_callbacks\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from copy import deepcopy\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "def build_hybrid_world_model_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Massage cfg.world_model into a DreamerV2Hybrid-compatible config.\n",
    "    Returns a new DictConfig (original cfg is untouched).\n",
    "    \"\"\"\n",
    "    # Resolve and clone the world-model block\n",
    "    wm = OmegaConf.create(deepcopy(OmegaConf.to_container(cfg.world_model, resolve=True)))\n",
    "    OmegaConf.set_struct(wm.loss, False)\n",
    "    wm.loss[\"state_weight\"] = wm.loss.get(\"state_weight\", 1.0)\n",
    "    OmegaConf.set_struct(wm.loss, True)\n",
    "    OmegaConf.set_struct(wm, False)\n",
    "\n",
    "    # Target the hybrid module\n",
    "    wm._target_ = \"lumos.world_models.dreamer_v2_hybrid.DreamerV2\"\n",
    "\n",
    "    # Drop legacy DreamerV2-only keys\n",
    "    wm.pop(\"batch_size\", None)\n",
    "    wm.pop(\"with_proprio\", None)\n",
    "    wm.pop(\"robot_dim\", None)\n",
    "    wm.pop(\"gripper_control\", None)\n",
    "\n",
    "    # Convenience handles\n",
    "    obs_cfg = cfg.datamodule.observation_space\n",
    "    proprio_cfg = cfg.datamodule.proprioception_dims\n",
    "    robot_dim = proprio_cfg.n_state_obs\n",
    "    scene_dim = 0  # extend if your observation space adds scene state\n",
    "    has_gripper_cam = \"rgb_gripper\" in obs_cfg.rgb_obs\n",
    "\n",
    "    # --- encoder -----------------------------------------------------------------\n",
    "    enc = wm.encoder\n",
    "    enc._target_ = \"lumos.world_models.encoders.cnn_mlp_encoder.CnnMLPEncoder\"\n",
    "    enc.use_gripper_camera = has_gripper_cam\n",
    "    enc.stride = enc.get(\"stride\", 2)\n",
    "    enc.robot_dim = robot_dim\n",
    "    enc.scene_dim = scene_dim\n",
    "    enc.state_out_dim = enc.get(\"state_out_dim\", 256)\n",
    "    enc.state_mlp_layers = enc.get(\"state_mlp_layers\", [512, 512])\n",
    "\n",
    "    # --- decoder -----------------------------------------------------------------\n",
    "    dec = wm.decoder\n",
    "    dec._target_ = \"lumos.world_models.decoders.cnn_mlp_decoder.CnnMLPDecoder\"\n",
    "    dec.stride = dec.get(\"stride\", 2)\n",
    "    dec.layer_norm = dec.get(\"layer_norm\", True)\n",
    "    dec.activation = dec.get(\"activation\", \"elu\")\n",
    "    dec.mlp_layers = dec.get(\"mlp_layers\", 0)\n",
    "    dec.state_mlp_layers = dec.get(\"state_mlp_layers\", [512, 512])\n",
    "    dec.use_gripper_camera = has_gripper_cam\n",
    "    dec.robot_dim = robot_dim\n",
    "    dec.scene_dim = scene_dim\n",
    "    dec.in_dim = enc.cnn_depth * 32  # visual embedding size\n",
    "    dec.in_dim += enc.state_out_dim   # plus state-MLP output\n",
    "\n",
    "    # The hybrid module sets rssm.cell.embed_dim internally; no manual tweak needed.\n",
    "\n",
    "    # --- training/AMP knobs -------------------------------------------------------\n",
    "    wm.train_batch_size = cfg.datamodule.batch_size\n",
    "    wm.val_batch_size = cfg.datamodule.batch_size\n",
    "    wm.use_gripper_camera = has_gripper_cam  # positional arg in DreamerV2Hybrid.__init__\n",
    "\n",
    "    OmegaConf.set_struct(wm.amp, False)\n",
    "    wm.amp.autocast._target_ = \"contextlib.nullcontext\"\n",
    "    wm.amp.autocast.pop(\"enabled\", None)\n",
    "    wm.amp.scaler.enabled = False\n",
    "    OmegaConf.set_struct(wm.amp, True)\n",
    "\n",
    "    OmegaConf.set_struct(wm, True)\n",
    "    return wm\n",
    "\n",
    "\n",
    "\n",
    "model_cfg = build_hybrid_world_model_cfg(cfg)\n",
    "model = hydra.utils.instantiate(model_cfg)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "cfg.exp_dir = str(project_root / \"logs\" / \"notebook\" / datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "cfg.logger = None\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "cfg.logger = None  # disable WandB\n",
    "csv_logger = CSVLogger(save_dir=str(project_root / \"logs\"), name=\"overfit_debug\")\n",
    "trainer = Trainer(logger=csv_logger, **cfg.trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fde979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DreamerV2(\n",
       "  (encoder): CnnMLPEncoder(\n",
       "    (activation): ELU(alpha=1.0)\n",
       "    (encoder_static): Sequential(\n",
       "      (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (encoder_gripper): Sequential(\n",
       "      (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (fuse): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "      (1): LayerNorm((1536,), eps=0.001, elementwise_affine=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (encoder_state): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=15, out_features=512, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "      )\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): CnnMLPDecoder(\n",
       "    (activation): ELU(alpha=1.0)\n",
       "    (decoder_static): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "      (1): Unflatten(dim=-1, unflattened_size=(1536, 1, 1))\n",
       "      (2): ConvTranspose2d(1536, 192, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): ConvTranspose2d(192, 96, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): ConvTranspose2d(96, 48, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): ConvTranspose2d(48, 3, kernel_size=(6, 6), stride=(2, 2))\n",
       "    )\n",
       "    (decoder_gripper): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "      (1): Unflatten(dim=-1, unflattened_size=(1536, 1, 1))\n",
       "      (2): ConvTranspose2d(1536, 192, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): ConvTranspose2d(192, 96, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): ConvTranspose2d(96, 48, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): ConvTranspose2d(48, 3, kernel_size=(6, 6), stride=(2, 2))\n",
       "    )\n",
       "    (decoder_state): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "      )\n",
       "      (2): Linear(in_features=512, out_features=15, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (rssm_core): RSSMCore(\n",
       "    (cell): RSSMCell(\n",
       "      (gru): GRUCellStack(\n",
       "        (layers): ModuleList(\n",
       "          (0): GRUCell(1000, 1024)\n",
       "        )\n",
       "      )\n",
       "      (z_mlp): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (a_mlp): Linear(in_features=7, out_features=1000, bias=False)\n",
       "      (in_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (prior_mlp_h): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (prior_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (prior_mlp): Linear(in_features=1000, out_features=1024, bias=True)\n",
       "      (post_mlp_h): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (post_mlp_e): Linear(in_features=1792, out_features=1000, bias=False)\n",
       "      (post_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (post_mlp): Linear(in_features=1000, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54cdea4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['robot_obs', 'rgb_obs', 'depth_obs', 'actions', 'state_info', 'lang', 'reset', 'frame', 'idx'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53347580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rgb_static', 'rgb_gripper'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"rgb_obs\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f104a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 15])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"robot_obs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "626bbbc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'state_obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_obs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state_obs'"
     ]
    }
   ],
   "source": [
    "batch[\"state_obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249d9adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['robot_obs', 'pre_robot_obs'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"state_info\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64aec45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DreamerV2(\n",
       "  (encoder): CnnEncoder(\n",
       "    (activation): ELU(alpha=1.0)\n",
       "    (encoder_static): Sequential(\n",
       "      (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (encoder_gripper): Sequential(\n",
       "      (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (fuse): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "      (1): LayerNorm((1536,), eps=0.001, elementwise_affine=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (decoder): CnnDecoder(\n",
       "    (activation): ELU(alpha=1.0)\n",
       "    (decoder_static): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "      (1): Unflatten(dim=-1, unflattened_size=(1536, 1, 1))\n",
       "      (2): ConvTranspose2d(1536, 192, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): ConvTranspose2d(192, 96, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): ConvTranspose2d(96, 48, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): ConvTranspose2d(48, 3, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (decoder_gripper): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "      (1): Unflatten(dim=-1, unflattened_size=(1536, 1, 1))\n",
       "      (2): ConvTranspose2d(1536, 192, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): ConvTranspose2d(192, 96, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): ConvTranspose2d(96, 48, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): ConvTranspose2d(48, 3, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (rssm_core): RSSMCore(\n",
       "    (cell): RSSMCell(\n",
       "      (gru): GRUCellStack(\n",
       "        (layers): ModuleList(\n",
       "          (0): GRUCell(1000, 1024)\n",
       "        )\n",
       "      )\n",
       "      (z_mlp): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (a_mlp): Linear(in_features=7, out_features=1000, bias=False)\n",
       "      (in_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (prior_mlp_h): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (prior_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (prior_mlp): Linear(in_features=1000, out_features=1024, bias=True)\n",
       "      (post_mlp_h): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (post_mlp_e): Linear(in_features=1551, out_features=1000, bias=False)\n",
       "      (post_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (post_mlp): Linear(in_features=1000, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=debug_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02efb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
